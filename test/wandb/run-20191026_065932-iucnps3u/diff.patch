diff --git a/sc2rl/__init__.py b/sc2rl/__init__.py
index 5d963ea..8b13789 100644
--- a/sc2rl/__init__.py
+++ b/sc2rl/__init__.py
@@ -1 +1 @@
-from .environments import *
\ No newline at end of file
+
diff --git a/sc2rl/config/__init__.py b/sc2rl/config/__init__.py
new file mode 100644
index 0000000..ffcdc51
--- /dev/null
+++ b/sc2rl/config/__init__.py
@@ -0,0 +1,4 @@
+from .ConfigBase import *
+from .graph_configs import *
+from .nn_configs import *
+from .unit_config import *
diff --git a/sc2rl/environments/__init__.py b/sc2rl/environments/__init__.py
index e69de29..7b57521 100644
--- a/sc2rl/environments/__init__.py
+++ b/sc2rl/environments/__init__.py
@@ -0,0 +1,3 @@
+__all__ = ['EnvironmentBase',
+           'MicroTestEnvironment',
+           'SC2BotAI']
\ No newline at end of file
diff --git a/sc2rl/memory/__init__.py b/sc2rl/memory/__init__.py
index e69de29..a41e60a 100644
--- a/sc2rl/memory/__init__.py
+++ b/sc2rl/memory/__init__.py
@@ -0,0 +1,4 @@
+__all__ = ['memory_base',
+           'n_step_memory',
+           'Trajectory',
+           'TrajectoryBase']
diff --git a/sc2rl/nn/__init__.py b/sc2rl/nn/__init__.py
index e69de29..bb4186f 100644
--- a/sc2rl/nn/__init__.py
+++ b/sc2rl/nn/__init__.py
@@ -0,0 +1,7 @@
+__all__ = ['AddNorm',
+           'FeedForward',
+           'Hypernet',
+           'MultiLayerPerceptron',
+           'RelationalNetwork',
+           'RelationalAttention',
+           'RelationalEncoder']
\ No newline at end of file
diff --git a/sc2rl/rl/__init__.py b/sc2rl/rl/__init__.py
index e69de29..fb847c9 100644
--- a/sc2rl/rl/__init__.py
+++ b/sc2rl/rl/__init__.py
@@ -0,0 +1,4 @@
+__all__ = ['agents',
+           'brains',
+           'modules',
+           'networks']
\ No newline at end of file
diff --git a/sc2rl/runners/__init__.py b/sc2rl/runners/__init__.py
index e69de29..2380e82 100644
--- a/sc2rl/runners/__init__.py
+++ b/sc2rl/runners/__init__.py
@@ -0,0 +1,3 @@
+__all__ = ['MultiStepActorRunner',
+           'RunnerBase',
+           'RunnerManager']
\ No newline at end of file
diff --git a/sc2rl/scripts/__init__.py b/sc2rl/scripts/__init__.py
index e69de29..8abafde 100644
--- a/sc2rl/scripts/__init__.py
+++ b/sc2rl/scripts/__init__.py
@@ -0,0 +1,3 @@
+import os
+import sys
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
diff --git a/sc2rl/scripts/train_multi_step_maac.py b/sc2rl/scripts/train_multi_step_maac.py
index 72285e0..6ef11bb 100644
--- a/sc2rl/scripts/train_multi_step_maac.py
+++ b/sc2rl/scripts/train_multi_step_maac.py
@@ -1,3 +1,8 @@
+import os
+import sys
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+
+
 import wandb
 import torch
 import numpy as np
diff --git a/sc2rl/tests/import_test.py b/sc2rl/tests/import_test.py
new file mode 100644
index 0000000..3dcb3cb
--- /dev/null
+++ b/sc2rl/tests/import_test.py
@@ -0,0 +1,5 @@
+from sc2rl.config.ConfigBase import ConfigBase
+
+if __name__ == "__main__":
+
+    conf = ConfigBase()
\ No newline at end of file
diff --git a/sc2rl/utils/__init__.py b/sc2rl/utils/__init__.py
index e69de29..7b7e890 100644
--- a/sc2rl/utils/__init__.py
+++ b/sc2rl/utils/__init__.py
@@ -0,0 +1,10 @@
+from .graph_utils import *
+from .reward_funcs import *
+
+# __all__ = ['graph_utils',
+#            'graph_indexing_test',
+#            'state_to_graph_utils',
+#            'HistoryManagers',
+#            'reward_funcs',
+#            'sc2_utils',
+#            'state_process_funcs']
\ No newline at end of file
diff --git a/test/__init__.py b/test/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/test/import_test.py b/test/import_test.py
new file mode 100644
index 0000000..e2c831e
--- /dev/null
+++ b/test/import_test.py
@@ -0,0 +1,10 @@
+import os
+import sys
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+
+import sc2rl
+from sc2rl.config.ConfigBase import ConfigBase
+
+if __name__ == "__main__":
+
+    conf = sc2rl.config.ConfigBase()
\ No newline at end of file
diff --git a/test/train_multi_step_maac.py b/test/train_multi_step_maac.py
new file mode 100644
index 0000000..6ef11bb
--- /dev/null
+++ b/test/train_multi_step_maac.py
@@ -0,0 +1,71 @@
+import os
+import sys
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+
+
+import wandb
+import torch
+import numpy as np
+import os
+
+from sc2rl.utils.reward_funcs import great_victor_with_kill_bonus
+from sc2rl.utils.state_process_funcs import process_game_state_to_dgl
+
+from sc2rl.rl.agents.MultiStepActorCriticAgent_refac import MultiStepActorCriticAgent, MultiStepActorCriticAgentConfig
+from sc2rl.rl.brains.MultiStepInputActorCriticBrain_refac import MultiStepActorCriticBrainConfig
+from sc2rl.rl.networks.MultiStepInputNetwork import MultiStepInputNetworkConfig
+
+from sc2rl.memory.n_step_memory import NstepInputMemoryConfig
+from sc2rl.runners.RunnerManager import RunnerConfig, RunnerManager
+
+if __name__ == "__main__":
+
+    map_name = "training_scenario_1"
+
+    agent_conf = MultiStepActorCriticAgentConfig()
+    network_conf = MultiStepInputNetworkConfig()
+    brain_conf = MultiStepActorCriticBrainConfig()
+    buffer_conf = NstepInputMemoryConfig()
+
+    sample_spec = buffer_conf.memory_conf['spec']
+    num_hist_steps = buffer_conf.memory_conf['N']
+
+    agent = MultiStepActorCriticAgent(agent_conf,
+                                      network_conf,
+                                      brain_conf,
+                                      buffer_conf)
+
+    config = RunnerConfig(map_name=map_name, reward_func=great_victor_with_kill_bonus,
+                          state_proc_func=process_game_state_to_dgl,
+                          agent=agent,
+                          n_hist_steps=num_hist_steps)
+
+    runner_manager = RunnerManager(config, 2)
+
+    wandb.init(project="sc2rl")
+    wandb.config.update(agent_conf())
+    wandb.config.update(network_conf())
+    wandb.config.update(brain_conf())
+    wandb.config.update(buffer_conf())
+    wandb.watch(agent)
+
+    iters = 0
+    while iters < 1000000:
+        iters += 1
+        runner_manager.sample(10)
+        runner_manager.transfer_sample()
+
+        fit_return_dict = agent.fit()
+
+        wrs = [runner.env.winning_ratio for runner in runner_manager.runners]
+        mean_wr = np.mean(wrs)
+
+        wandb.log(fit_return_dict, step=iters)
+        wandb.log(fit_return_dict, step=iters)
+        wandb.log({'winning_ratio': mean_wr}, step=iters)
+
+        if iters % 50 == 0:
+            save_path = os.path.join(wandb.run.dir, "model_{}".format(iters))
+            torch.save(agent.state_dict(), save_path)
+
+    runner_manager.close()
